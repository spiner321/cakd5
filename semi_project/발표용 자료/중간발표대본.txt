안녕하세요 CAKD5 광민아노래추천해조 팀의 크롤링 프로젝트 중간 발표를 진행하도록 하겠습니다. 

발표 내용은 기획 배경, 과제 도출, 데이터 수집 및 전처리, 진전상황 순으로 말씀드리겠습니다.  

"헤이 광민아, 발라드 틀어줘" (우울한 목소리로) 
"기분이 안 좋아 보이네요. 기분 전환 겸 신나는 댄스곡이나 힙합은 어떠세요?" (AI 같은 목소리로) 
혹시 학원에서 열심히 공부한 뒤 다소 지쳐서 집으로 돌아가는 길 음악을 들으며 힐링한 적 경험이 있으신가요?
다운된 하루 어떤 노래를 들어야 할지 당신의 목소리와 얼굴을 파악해서 AI 스피커가 취향에 맞는 음악을 추천해준다면 어떨까요? 
저희의 프로젝트는 내 감정에 공감할 수 있는 AI 스피커가 상용화된다면 어떨까라는 생각에서 시작되었습니다. 

지금까지의 인공지능(AI) 스피커는 대화형 AI인 챗봇이나 음성인식 AI로서 콜센터와 고객센터의 상담지원시스템, 집안 가전과
연동된 사물인터넷(loT) 등으로 활용되어 왔습니다. 
하지만 인간 의사소통의 90%는 발성 억양, 몸짓 언어, 얼굴 표정을 포함하기 때문에 
단순히  음성 인식만 사용하는 AI는 감성적인 부분은 놓치고 있어서 한계로 지적되고 있습니다.

따라서 사물인터넷에 관한 관심이 높아지고 있음에도 불구하고, 아직은 사물인터넷 서비스에 대한 이용자들의 
만족도가 낮은 것은 당연한 것으로 보입니다. 

저희 조는 이러한 한계를 개선하고자 이번 프로젝트의 과제로 최근에 인간의 감정을 이해하고 반응하는 인공감성지능 구현 기술에 집중을 해보았습니다. 
저희가 구현하고자 하는 기술은 사용자의 얼굴 표정과 목소리 톤으로 ai 스피커가 사용자의 감정 상태에 걸맞는 음악을 추천해주는 것입니다. 

지금 저희가 구현할 기술은 단순 스피커에 포커스가 맞춰져있지만 이는 공감 AI 기술로 확장될 수 있습니다.
공감 AI 기술을 활용한 서비스의 예상 기대효과로는 
1. 사용자 감정을 감지하고 반응할 수 있는 '감정적인' AI는 인간과 AI 간 협력 노력에서 이용자의 만족도를 높일 수 있다는 것입니다.

2. 그리고 사용자들의 신뢰를 얻어 더욱 효과적인 상호작용을 이끄는 것은 물론이고 시스템이 잠재적 문제를 인식하도록 도울 수 있습니다. 
예를 들자면, 사용자가 우울증 등으로 좌절감이 높아졌을 때, AI는 사용자에게 문제가 있다는 신호를 인식하고 솔루션을 제공해 문제를 해결할 수 있습니다. 


다음으로 서비스에 필요한 데이터 수집 과정에 대해 말씀해드리겠습니다. 
AI HUB에서 감성 대화 말뭉치 데이터를 다운로드 받습니다.  그리고 이 데이터를 LIBROSA 라이브러리를 통해 음성의 파형으로 감정을 구분할 수 있도록
학습시켜 사용할 예정입니다. 이 학습모델은 외부에서 이미 구현한 학습모델을 참조하여 사용하기로 하였습니다.
다음으로 필요한 입력 값인 이미지는 별도 데이터 학습없이
텐서플로우의 얼굴 표정 인식(Facial Expression Recognition,FER) 시스템으로 이미지나 동영상 인식을 인식하는 기술을 사용할 예정입니다. 
그리고 출력값인 음악은 공공데이터포털 음악 저작물에서 아티스트명, 앨범명, 노래제목 수집 후 데이터프레임으로 작성하고 
데이터 프레임에 들어있는 음악의 음원을 크롤링을 통해 추출해 파형을 분석할 예정입니다. 
음악에서 감성을 추출할 때도 librosa 라이브러리를 사용할 예정입니다. 저희가 참고한 음원감성분석 연구에서는 matlab mirtoolbox를 사용하여서 
원래는 이 프로그램을 사용할 예정이었으나 아직 matlab을 완벽히 다루기가 힘들고 librosa의 기능을 찾아보니 librosa 만으로도 음원감정분석에
 사용할 수 있는 데이터 정도는 뽑아낼 수 있다고 판단하여 이번 프로젝트 상에서는 librosa를 통해 구현해볼 예정입니다. 
librosa는 앞서 음성인식 감정분석에도 사용하고 있고 파이썬 호환성이 높은 점도 선택한 이유 중 하나 입니다.

(((((((((((((다음으로는 이번 프로젝트에서 사용할 기술에 대해서 좀 더 설명해드리도록 하겠습니다. 
앞서 말씀드린 librosa는 파이썬에서 음원데이터를 분석해주는 라이브러리로 이를 사용해 파형을 불러올 수 있습니다. 
fer( Facial Expression Recognition)는 CNN 모델을 이용하여 얼굴 표정을 인식하는 모델입니다.
CNN은 Convolutional Neural Networks의 약자로 딥러닝에서 주로 이미지나 영상 데이터를 처리할 때 쓰이는 모델입니다.
angry, disgusted, fearful, happy, sad, suprised, neutral 총 7개의 표정을 인식합니다. 
현재는 fer를 이용한 사진 및 영상 이미지 감성분석을 진행해보았지만 ms에서 제공하고 있는 face api를 사용하여 두 기능을 비교한 뒤
좀 더 정확도가 높은 기술을 채택할 예정입니다. ))))))))))))))

다음은 현재 저희의 진전 상황입니다. 
우선 공공데이터 포털 음악 저작물을 크롤링하여 데이터 프레임으로 작성했습니다. 
또한 FER를 통해 이미지나 영상 데이터를 입력 한 뒤 감성 분석을 해보았습니다. 
다양한 상황의 이미지를 부여하고 감정 상태를 알맞게 인식할 수 있는지 테스트 해보았습니다.

저희의 앞으로 계획은 AI HUB의 말뭉치 데이터를 횔용하여 LIBROSA를 통해 입력받은 목소리 톤으로 감정을 나눌 수 있게 학습시킬 예정입니다.
그리고 음원을 멜론과 유튜브에서 추출하여 파형을 분석해 8가지 감정상태별로 나눠볼 예정입니다. 

저희 조가 주제 선정을 이번 주 화요일 다시 하게 되여 아직 부족한 점이 많지만 다음 주 월요일 최종 발표까지 열심히 보완해 볼 예정입니다. 감사합니다. 



