분류모델 8가지
1. Decision Tree, 데이터 균일도에 따른 규칙 기반의 결정 트리, 과적합의 단점, 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다. 정보 균일도의 측정은 정보이득지수(1 - 엔트로피)와 지니 불순도를 기준으로 삼는다.

장점 : 정보의 균일도만 신경쓰면 되므로 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화 같은 전처리 작업이 필요가 없다. 정보의 균일도를 기반으로 하는 알고리즘이므로 직관적이며 어떻게 노드가 만들어지는 알 수 있고 시각화 할 수 있다.
(DecisionTreeClassifier - 분류, DecisionTreeRegressor - 회귀)

단점 : 과적합으로 정확도가 떨어진다 따라서 사전에 트리의 크기를 제한해줘야 한다.

결정트리 파라미터: min_samples_split, min_samples_leaf, max_features, max_depth, max_leaf_nodes

2. KNN(k nearest neighbor), 새로운 데이터에서의 거리가 가까운 k개의 다른 데이터의 레이블을 참고하여 가장 빈도 수가 높게 나온 레이블로 분류하는 알고리즘 , 보편적으로 k 값은 동률이 나오지 않도록 홀수로 지정 

주의점: 변수의 중요도를 고르게 해석하기 위해서 변수들의 값 범위를 재조정 해주어야 한다. 최소-최대 정규화나 z점수 표준화를 이용할 수 있다.

3. SVM(SVC), 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신
svm은 분류(SVC), 회귀(SVR), 특이점 판별(OneClassSVM)에 쓰이는 지도학습 머신러닝 방법 
점선으로 두 그룹을 분리했을 때 각 그룹이 최대로 떨어질 수 있는 최대 거리(최대 마진)를 찾고 해당 거리의 중간지점(초평면)으로 각 그룹을 구분짓는 기법, 커널의 종류로는 poly(다항커널), rbf(가우시안 커널), sigmoid(시그모이드 커널)이 있다.

4. Logistic Regression, 독립변수와 종속변수의 선형 관계성에 기반
회귀를 사용하여 데이터가 어떤 범주에 속할 확률을 0에서 1 사이의 값으로 예측하고 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘이다.

각 속성들의 계수 로그오즈를 구한 후 시그모이드 함수를 적용하여 실제로 데이터가 해당 클래스에 속할 확률을 0~1사이의 값으로 나타낸다. 손실함수를 통해 모델이 얼마나 잘 예측하는지 확인한다. 

5. random forest, 배깅과 부스팅으로 나눠지는 앙상블에서 대표적인 배깅 방식 알고리즘
배깅은 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘, 랜덤 포레스트의 기반 알고리즘은 결정 트리로서 결정 트리의 쉽고 직관적인 장점을 그대로 가지고 있음

랜덤 포레스트 하이퍼 파라미터: n_estimators, max_features, max_depth, min_samples_leaf

6. GBM, 부스팅은 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치를 부여하면서 학습과 예측을 진행하는 것, gradient boosing 가중치 업데이트를 경사하강법(Gradient Descent)을 이용함 

GBM 하이퍼 파라미터 : loss, learning_rate, n_estimators, subsample

7. XGBoost, extra gradient boost, GBM에 기반하고 있지만, GBM의 단점인 느린 수행 시간 및 과적합 규제 부재 등의 문제를 해결, 병렬 CPU 환경에서 병렬 학습이 가능하다.

XGBClassifier 하이퍼 파라미터 : learning_rate, subsample, reg_lambda, reg_alpha 
조기 중단 관련 파라미터 : early_stopping_rounds, eval_metirc, eval_set 
 
8. Light GBM
Light GBM은 일반 GBM 계열의 균형 트리 분할 방법과 다르게 최대 손실 값을 가지는 리프 노드 중심 트리 분할 방식을 사용함 따라서 비대칭적인 규칙 트리가 형성됨
장점: XGBoost보다 학습에 걸리는 시간이 훨씬 적음, 메모리 사용량도 상대적으로 적음, 
카테고리형 피처의 자동변환과 최적 분할(원-핫 인코딩을 사용하지 않아도 됨) 
단점: 10,000건 이하의 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉽다 

Light GBM 하이퍼 파라미터: num_leaves, min_child_samples, max_depth