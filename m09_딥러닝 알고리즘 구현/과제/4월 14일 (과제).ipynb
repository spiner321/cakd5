{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4월 14일 (과제).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOI61AqaWW1DFAaJEntAQZy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##### [과제] 사전 훈련된 임베딩을 사용하지 않고 상기 작업(샘플 200개)을 수행하여 비교해 보세요.\n","---"],"metadata":{"id":"_jvhRgKLGzSb"}},{"cell_type":"markdown","source":["완성한 데이터는 4월 13일 ~ 4월 14일 (Word Embedding).ipynb에 있음"],"metadata":{"id":"Wuz1ZmpqHaPX"}},{"cell_type":"markdown","source":["##### [과제] 훈련 샘플 수를 2000개로 늘려서 모델링 및 시각화 수행하여 상기 경우와 비교해 보세요.\n","---"],"metadata":{"id":"cIKxuiWEG2Sb"}},{"cell_type":"markdown","source":["완성한 데이터는 4월 13일 ~ 4월 14일 (Word Embedding).ipynb에 있음"],"metadata":{"id":"Gfg3Cj71HcOV"}},{"cell_type":"markdown","source":["##### [과제] The Project Gutenberg 사이트 텍스트를 전처리 후 다음을 수행하세요.\n","---\n","- 텍스트 : https://www.gutenberg.org/files/2591/2591-0.txt\n","- 사전 훈련된 Word2Vec을 gensim을 이용하여 임포트 후 cbow, skip-gram 방식으로 모델링하여 유사도 측정\n","  - wv.similarity('king','prince')\n","  - wv.most_similar('king')\n","  - most_similar(positive=['man','princess'], negative=['woman'])\n","- 옵션 사항\n","  - size(워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원) : 100\n","  - window(컨텍스트 윈도우 크기) : 3\n","  - min_count(단어 최소 빈도 수 제한-빈도가 적은 단어들은 학습하지 않는다.) : 3\n","  - workers(학습을 위한 프로세스 수) : 4\n","  - sg = 0은 CBOW, 1은 Skip-gram.\n"],"metadata":{"id":"oB04XjfxwKfy"}},{"cell_type":"markdown","source":["완성한 데이터는 4월 14일 (Word2Vec).ipynb에 있음"],"metadata":{"id":"u2Ou2j0MHetI"}}]}